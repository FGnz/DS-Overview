{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d33d1f-53a2-4984-b9d8-bcac79eb7bc2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Author:<br>Felix Gonzalez, P.E. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef001aa-5ab0-47ac-b366-7716d8faab44",
   "metadata": {},
   "source": [
    "Short intro goal scope and Important Citations and References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08bdac-b6ba-46cd-b784-dbd64fc981e7",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[](#)\n",
    "\n",
    "[](#)\n",
    "\n",
    "[](#)\n",
    "\n",
    "[](#)\n",
    "\n",
    "[](#)\n",
    "\n",
    "[](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11dca4-174b-45f5-aea9-23a0e03ec9fb",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning: Classification\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "When designing a system to detect and recognize images, the approach used is a supervised machine learning classification approach. In this approach a set of records (i.e., in this case images) is collected and labeled. The labels are then used to train (i.e., teach) the model what specific labels look like. Before a model is deployed the model is tested with part of the labeled data to measure how well the model performed by calculating various performance metrics (e.g., accuracy, precision, recall, etc.). Once the model is trained and tested, we can proceed to deployment.\n",
    "\n",
    "There may also be various pre-trained models (e.g., YOLO) that can be used \"off the shelf\" without having to go through data collection, labeling, training and testing. Most of these models will also allow to further add use case specific custom data to allow for detection and classification in specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced74ae-4fe9-4e14-8dfb-57d5a1ca1f41",
   "metadata": {},
   "source": [
    "# Classification Model Steps\n",
    "\n",
    "Classification problems (e.g., image classification) the goal is to predict a categorical or class for a picture or image or part of an image. To accomplish this prediciton, there are various staeps that are performed when developing and evaluating a supervised ML classification model. These steps include as follows:\n",
    "\n",
    "- Defining values of the independent variable or variables (x)\n",
    "- Defining values of the dependent variable or variable to be predicted (y)\n",
    "- Scaling/normalization if needed\n",
    "- Train/Test Split (Typically 80/20)\n",
    "- Fitting the training data to the model\n",
    "- Calculate predictions using the testing data.\n",
    "- The data may sometimes be divided into training, testing and validation.\n",
    "- Calculating model performance metrics\n",
    "    - For classification model common metrics are confusion matrix, sensitivity, specificity, precision, recall, f-measure, true-positive rate, false-positvie rate, null-accuracy, etc.)\n",
    "    - The testing data may be used for parameter optimization and then the validation data for calculating final performance metrics.-  \n",
    "- Deploy and use the model for predicting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ad8d3-206d-46d4-b550-aac89c1984b3",
   "metadata": {},
   "source": [
    "# Image Classification Components\n",
    "\n",
    "In image classification there are several components that are needed that may vary from other types of classification problems and applications. For example,\n",
    "- Annotated and labeled images of what is being detected and that will be used to train and teach the model what we need to identify. Alternatively, a pre-trained model may already have defined classes that can be used off the shelf for image detection.\n",
    "- Mathematical model, methods, and approaches with the ability to read image and video inputs\n",
    "- Data input for classification:\n",
    "    - Collection of files (i.e., images)\n",
    "    - Video feed (e.g., camera)\n",
    "\n",
    "#  Image Collection and Capture\n",
    "\n",
    "Image collection can be performed in multiple ways and here are some potential approaches:\n",
    "- One approach is to collect pictures manually (i.e., with a camera) and put them in the respective folder.\n",
    "- In the tutorial and code from Nicholas Renotte it is performed by defining a number of images (i.e., 5) and there is a loop that iterates through every label and using the laptop camera collects the predetermined number of pictures.\n",
    "- Another approach could be to develop a widget that allows you to perform such task.\n",
    "\n",
    "Change image collection to a Widget that uses the labels but also allows creation of new label. The widget should have a button to take picture and then a button to open the labelimg application.\n",
    "\n",
    "Once images have the bounding box, and are annotated and labeled, we need to divide each image set for each label between the training/test folders. Ideally as close to possible to an 80% of the pictures for each label into training folder and 20% of pictures for each label into the testing folder. These can be done using various approaches:\n",
    "- For each label, manually move each image/annotation file pairs to training and testing folder following 80/20 rule.\n",
    "- Create an algorithm that randomly picks image/annotation file pairs to training and testing folder following 80/20 rule. \n",
    "\n",
    "The images in the training folder will later allow us to teach the model which images are which label. The images in the testing data will allow us to calculate performance metrics and know how well our model can recognize each image. Model performance metrics include true positives/negatives and false positives negatives which are represented in confusion matrix.\n",
    "\n",
    "From a confusion matrix other important performance metric can be calculated such as the precision, recall, specificity, sensitivity, loss, null accuracy and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c4aec-52f3-43d3-8ded-75e61c06ca77",
   "metadata": {},
   "source": [
    "These set of notebooks where inspired by the solutions found in the following references:\n",
    "- https://docs.opencv.org/4.x/d2/d58/tutorial_table_of_content_dnn.html\n",
    "- https://docs.ultralytics.com/models/yolov8\n",
    "- https://medium.com/@abauville/display-your-live-webcam-feed-in-a-jupyter-notebook-using-opencv-d01eb75921d1\n",
    "- https://www.udemy.com/course/yolov8-the-ultimate-course-for-object-detection-tracking/learn/lecture/37260958#overview\n",
    "\n",
    "Other references utilized include as follows:\n",
    "\n",
    "- https://github.com/chandrikadeb7/Face-Mask-Detection/blob/master/detect_mask_video.py\n",
    "\n",
    "Jetson Nano Image Recognition References:\n",
    "- https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs\n",
    "- https://github.com/dusty-nv/jetson-inference?tab=readme-ov-file\n",
    "- https://github.com/dusty-nv/jetson-inference/blob/master/docs/detectnet-example-2.md\n",
    "- https://www.youtube.com/watch?v=zsjcSapzUfU\n",
    "- https://www.youtube.com/watch?v=rSqIvLQ8Meg\n",
    "- https://www.youtube.com/watch?v=LMsUP-W-3FI\n",
    "\n",
    "Other References:\n",
    "- https://medium.com/@amit25173/opencv-object-recognition-642c8cf8379b\n",
    "- https://www.geeksforgeeks.org/detect-an-object-with-opencv-python/\n",
    "- https://www.youtube.com/watch?v=bUoWTPaKUi4\n",
    "- https://www.youtube.com/watch?v=yqkISICHH-U&t=9438s\n",
    "- https://github.com/nicknochnack/TFODCourse\n",
    "- https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/object_detection_camera.html\n",
    "- https://pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/\n",
    "- https://www.kaggle.com/code/ahmedmahmoud16/facial-expression-recognition-with-logistic\n",
    "- https://www.kaggle.com/code/sharadhaviswanathan/imageclassification-facialexpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410babc-c097-4243-b6d8-29b8af8f211b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4d647-2c4f-47b0-8666-83f45d175cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed60775-2c1b-4ebe-9fc7-5794a2a81bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923bb00-03e6-43e0-b320-f3134ff57726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe322ce-692d-4134-9396-4f8ca68a24a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a0946-581d-49d8-902a-74d2b9cff3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58587262-cea7-453c-83a4-3643e9455799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eda5e2-312d-4f19-9a1a-04e388c58d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54805535-2bf9-4609-b60f-1cc57bf033c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4b5f0-8fc9-4dde-87ba-81f80adb09b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6acb9c-0599-4d06-a7b0-54467ab65134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4d3ff-0f84-45ce-a354-fb8bbf05ccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56db8e-ea34-4256-9c6e-787b495ce7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80574b4b-2d93-42bf-9f20-cd0180e5dc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
